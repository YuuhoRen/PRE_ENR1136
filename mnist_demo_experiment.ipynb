{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b63a000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "torch.__version__\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3b80812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db1: 8000 db2: 2000\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "traindb=datasets.MNIST('data', train=True, download=True, #cifar10数据集是经典数据集，可以直接载入，第一次使用需要下载。\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(), # 转换成torch.tensor\n",
    "                       ]))\n",
    "\n",
    "train_db, val_db, drop_db= torch.utils.data.random_split(traindb, [8000, 2000,50000]) #这里将数据集分成了三份，为了看网络在小样本数据下的性能，\n",
    "#（接上一行）这里将40000个数据直接扔掉了，5000个训练数据，5000个验证数据，这三个加起来，要等于总的数据50000个。\n",
    "# train_db, val_db= torch.utils.data.random_split(traindb, [40000, 10000])\n",
    "\n",
    "print('db1:', len(train_db), 'db2:', len(val_db))\n",
    "train_loader = torch.utils.data.DataLoader(   #搞出一个训练集\n",
    "    train_db,\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(     #搞出一个验证集\n",
    "    val_db,\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b995cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customF(nn.Module): #咱们的激活函数，阈值+噪声\n",
    "\n",
    "\n",
    "    def __init__(self, width):\n",
    "        super().__init__()\n",
    "        self.sigma = nn.Parameter(torch.rand(1, width, 1, 1)) #两个可学习参数 sigma 和 theta\n",
    "        self.theta = nn.Parameter(torch.rand(1, width, 1, 1))\n",
    "\n",
    "    def forward(self, input):\n",
    "      #  temp1=self.sigma*torch.exp(-input*input/2/self.sigma/self.sigma)/2.506628\n",
    "        temp=0.5-torch.erf((self.theta-input)/self.sigma/1.4142136)/2 #前向传播过程\n",
    "      #  return temp1+temp2\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a37453b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customFC(nn.Module): #咱们的激活函数，阈值+噪声\n",
    "\n",
    "\n",
    "    def __init__(self, width):\n",
    "        super().__init__()\n",
    "        self.sigma = nn.Parameter(torch.randn(1, width)) #两个可学习参数 sigma 和 theta\n",
    "        self.theta = nn.Parameter(torch.randn(1, width))\n",
    "\n",
    "    def forward(self, input):\n",
    "      #  temp1=self.sigma*torch.exp(-input*input/2/self.sigma/self.sigma)/2.506628\n",
    "        temp=0.5-torch.erf((self.theta-input)/self.sigma/1.4142136)/2 #前向传播过程\n",
    "      #  return temp1+temp2\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d094b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convdense = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16)\n",
    "            \n",
    "        )\n",
    "        self.customdense = customF(16)\n",
    "\n",
    "        self.fc1 = nn.Linear(3136, 256) \n",
    "        self.customFc1 = customFC(256)\n",
    "        self.fc2 = nn.Linear(256, 10) \n",
    "    def forward(self,x):\n",
    "        in_size = x.size(0) \n",
    "        out = self.convdense(x)\n",
    "        \n",
    "        out = self.customdense(out) \n",
    "        \n",
    "        out = torch.max_pool2d(out, 2, 2) \n",
    "\n",
    "               \n",
    "        out = out.view(in_size, -1) \n",
    "        out = self.fc1 (out) \n",
    "        out = self.customFc1(out) \n",
    "        out = self.fc2(out) \n",
    "        out = F.log_softmax(out, dim=1) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "08de37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module): #激活函数为我们构造的激活函数。\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1) \n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.custom1 = customF(16)\n",
    "    \n",
    "        #self.conv2 = nn.Conv2d(16, 32, 3,padding=1) \n",
    "        #self.custom2 = customF(32)\n",
    "     \n",
    "        #self.fc1 = nn.Linear(3136, 256) \n",
    "       # self.customFc1 = customFC(256)\n",
    "        self.fc2 = nn.Linear(3136, 10) \n",
    "    def forward(self,x):\n",
    "        in_size = x.size(0) \n",
    "        out = self.conv1(x) \n",
    "        out = self.bn1(out)\n",
    "        out = F.max_pool2d(out, 2, 2)     \n",
    "        out = self.custom1(out,20) \n",
    "        \n",
    "        \n",
    "        #out = self.conv2(out) \n",
    "       # out = self.custom2(out,32) \n",
    "        #out = F.max_pool2d(out, 2, 2)\n",
    "               \n",
    "        out = out.view(in_size, -1) \n",
    "        #out = self.fc1(out) \n",
    "        #out = self.customFc1(out)\n",
    "         \n",
    "        out = self.fc2(out) \n",
    "        out = F.log_softmax(out, dim=1) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04f0eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module): #激活函数为 Sigmoid 函数  最后分类层为softmax。\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1) \n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        #self.custom1 = customF(16)  \n",
    "    \n",
    "        #self.conv2 = nn.Conv2d(6, 32, 3,padding=1) \n",
    "        #self.custom2 = customF(32)\n",
    "     \n",
    "        self.fc1 = nn.Linear(3136, 256) \n",
    "        self.fc2 = nn.Linear(256, 10) \n",
    "    def forward(self,x):\n",
    "        in_size = x.size(0) \n",
    "        out = self.conv1(x) \n",
    "        out = self.bn1(out) \n",
    "        out = F.sigmoid(out)\n",
    "        out = F.max_pool2d(out, 2, 2) \n",
    "        \n",
    "        \n",
    "        #out = self.conv2(out) \n",
    "       # out = self.custom2(out,32) \n",
    "        #out = F.max_pool2d(out, 2, 2)\n",
    "               \n",
    "        out = out.view(in_size, -1) \n",
    "        out = self.fc1(out) \n",
    "        out = F.sigmoid(out)\n",
    "        #out = F.relu(out) \n",
    "        out = self.fc2(out) \n",
    "        out = F.log_softmax(out, dim=1) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc710355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module): ##激活函数为 Relu 函数  最后分类层为softmax。\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1) \n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        #self.custom1 = customF(16)\n",
    "    \n",
    "        #self.conv2 = nn.Conv2d(6, 32, 3,padding=1) \n",
    "        #self.custom2 = customF(32)\n",
    "     \n",
    "        self.fc1 = nn.Linear(3136, 256) \n",
    "        self.fc2 = nn.Linear(256, 10) \n",
    "    def forward(self,x):\n",
    "        in_size = x.size(0) \n",
    "        out = self.conv1(x) \n",
    "        out = self.bn1(out) \n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 2, 2) \n",
    "        \n",
    "        \n",
    "        #out = self.conv2(out) \n",
    "       # out = self.custom2(out,32) \n",
    "        #out = F.max_pool2d(out, 2, 2)\n",
    "               \n",
    "        out = out.view(in_size, -1) \n",
    "        out = self.fc1(out) \n",
    "        out = F.relu(out) \n",
    "        out = self.fc2(out) \n",
    "        out = F.log_softmax(out, dim=1) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af51271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 600\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS=100# 总共训练批次\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "#DEVICE = (\"cpu\")\n",
    "model = ConvNet().to(DEVICE)\n",
    "#model = ConvNet().cuda()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001,betas=(0.96,0.96))\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum=0.8) #这里用的是SGD算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fccb0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        para1[epoch-1]=loss.data\n",
    "        \n",
    "       # if(batch_idx+1)%125 == 0:   #训练集5000张图片，batchsize=64，每epoch迭代约78次，可以39次迭代验证一次。\n",
    "       #         print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #        epoch, batch_idx * len(data), len(train_loader.dataset),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
    "        #        100. * batch_idx / len(train_loader), loss.item()))\n",
    "        \n",
    "   # print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,loss.item()))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    loss.item()))\n",
    "    print(epoch,loss.item())\n",
    "    \n",
    "    valid(model, DEVICE, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7708896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, device, valid_loader):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            valid_loss += F.nll_loss(output, target, reduction='sum').item() # 将一批的损失相加\n",
    "            pred = output.max(1, keepdim=True)[1] # 找到概率最大的下标\n",
    "            valid_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    para2[epoch-1]= valid_correct\n",
    "    print('Valid set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "         valid_loss, valid_correct, len(val_loader.dataset),\n",
    "         100. * valid_correct / len(val_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3ae3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # 将一批的损失相加\n",
    "            pred = output.max(1, keepdim=True)[1] # 找到概率最大的下标\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    #para3[epoch-1]=test_loss.data\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d78dc0ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'width'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10488/1965985936.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#在所有的可学习参数里\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10488/4035967581.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10488/165198347.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcustomdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'width'"
     ]
    }
   ],
   "source": [
    "para1=torch.zeros(EPOCHS)\n",
    "para2=torch.zeros(EPOCHS)\n",
    "\n",
    "para3=torch.tensor([])\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    if epoch%1==0:\n",
    "        for name, param in model.named_parameters(): #在所有的可学习参数里\n",
    "            if param.requires_grad:\n",
    "                if name=='custom1.sigma': #找到sigma\n",
    "                    temppara1=param.data  #记下来\n",
    "                    a=temppara1.reshape(-1,1)\n",
    "                   # print(a)\n",
    "                    para3=torch.cat([para3,a],dim=0) #存到para1的末尾\n",
    "\n",
    "    \n",
    "    \n",
    "#test(model, DEVICE, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82278a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "expCorrect = 0 #这个程序段实现了实验测试，val_db为测试集，用的激活函数是M个子系统并联，加入高斯噪声的\n",
    "\n",
    "picNumber = len(val_db)  # 测试数据的个数\n",
    "M=3  # 并联阈值函数的个数\n",
    "nodeN=16   # 第一层卷积核的个数\n",
    "nodeFC=3136\n",
    "picSample,_ = val_db[0]\n",
    "height = picSample.size()[1]  # 提取图片的高\n",
    "width = picSample.size()[2]   # 提取图片的宽\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for kk in range (picNumber):\n",
    "\n",
    "        tempimg, templabel= val_db[kk]  # 提取测试图片及标签\n",
    "\n",
    "        label2=torch.tensor(templabel, dtype=float)\n",
    "        label2 = torch.unsqueeze(label2,dim=0) \n",
    "\n",
    "        tempimg2 = torch.unsqueeze(tempimg,dim=0)\n",
    "\n",
    "        outAfterConv=model.convdense(tempimg2.to(DEVICE)) #xx,16,28,28\n",
    "\n",
    "    \n",
    "        tempAfterCustom1=torch.zeros(1, nodeN, height, width).to(DEVICE)\n",
    "        for jj in range (nodeN):     \n",
    "            fig1 = outAfterConv[:,jj,:,:].expand(1, M, height, width)\n",
    "            noiseEta = model.customdense.sigma[0,jj,0,0]*torch.randn(1,M, height, width).to(DEVICE)\n",
    "            thres = model.customdense.theta[0,jj,0,0].to(DEVICE)\n",
    "            fig2 = 0.5*(torch.sign(fig1+noiseEta+thres)+torch.sign(fig1+noiseEta-thres))\n",
    "            #fig2 = torch.sign(fig1+noiseEta-thres)\n",
    "            fig3 = torch.mean(fig2,dim=1,keepdim=True)\n",
    "            tempAfterCustom1[:,jj,:,:] = fig3.view(1, height, width)\n",
    "\n",
    "        outAfterCustom1 = tempAfterCustom1\n",
    "        outAfterPool=torch.max_pool2d(outAfterCustom, 2, 2)\n",
    "        outBeforeFc = outAfterPool.view(1, -1) \n",
    "        \n",
    "        tempAfterCustom1=torch.zeros(nodeFC, 1).to(DEVICE)\n",
    "        for jj in range (nodeFC):     \n",
    "            noiseEta = model.customdense.sigma[0,jj,0,0]*torch.randn(1,M, height, width).to(DEVICE)\n",
    "            thres = model.customdense.theta[0,jj,0,0].to(DEVICE)\n",
    "            fig2 = 0.5*(torch.sign(fig1+noiseEta+thres)+torch.sign(fig1+noiseEta-thres))\n",
    "            #fig2 = torch.sign(fig1+noiseEta-thres)\n",
    "            fig3 = torch.mean(fig2,dim=1,keepdim=True)\n",
    "            tempAfterCustom1[:,jj,:,:] = fig3.view(1, height, width)\n",
    "\n",
    "        \n",
    "        \n",
    "        outAfterFc1 = model.fc1(outBeforeFc)\n",
    "        \n",
    "        \n",
    "        outAfterFc = model.fc2(outBeforeFc)\n",
    "\n",
    "        outExp=F.log_softmax(outAfterFc, dim=1)\n",
    "\n",
    "        expPred = outExp.max(1,keepdim=True)[1]\n",
    "        expCorrect += expPred.eq(label2.view_as(expPred).to(DEVICE)).sum().item()\n",
    "#    print(expCorrect/picNumber)\n",
    "\n",
    "print('Experimental Test Accuracy: {}/{} ({:.2f}%)\\n'.format(expCorrect, picNumber,\n",
    "        100. * expCorrect / picNumber))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea76ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(para3)   #储存每一步的sigma\n",
    "frame1=pd.DataFrame(para3.data.numpy()) \n",
    "frame1.to_csv('C:/results/sigma.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1571e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#注意！上面的程序用验证集充当测试集。如果要用mnist的测试集，用下面的代码，然后把上一个模块中的测试代码test(model, DEVICE, val_loader)变成tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57c1c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=pd.DataFrame(para1.data.numpy()) \n",
    "frame1.to_csv('C:/results/loss_train011.csv',index=False)\n",
    "frame1=pd.DataFrame(para2.data.numpy()) \n",
    "frame1.to_csv('C:/results/valid_correct011.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "205a7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=pd.DataFrame(para1.data.numpy()) \n",
    "frame1.to_csv('C:results/loss_traindl.csv',index=False)\n",
    "frame1=pd.DataFrame(para2.data.numpy()) \n",
    "frame1.to_csv('C:results/valid_correctdl.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59a7a61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "conv1.bias\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "custom1.sigma\n",
      "custom1.theta\n",
      "fc1.weight\n",
      "fc1.bias\n",
      "customFc1.sigma\n",
      "customFc1.theta\n",
      "fc2.weight\n",
      "fc2.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print (name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b764da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=pd.DataFrame(model.fc1.bias.data.numpy())  #存每个epoch的sigma和theta\n",
    "\n",
    "frame1.to_csv('C:/results/fc1bias.csv',index=False) \n",
    "frame2=pd.DataFrame(model.fc1.weight.data.numpy())  #存每个epoch的sigma和theta\n",
    "frame2.to_csv('C:/results/fc1weight.csv',index=False) \n",
    "frame3=pd.DataFrame(model.fc2.bias.data.numpy())  #存每个epoch的sigma和theta\n",
    "frame3.to_csv('C:/results/fc2bias.csv',index=False) \n",
    "frame4=pd.DataFrame(model.fc2.weight.data.numpy())  #存每个epoch的sigma和theta\n",
    "frame4.to_csv('C:/results/fc2weight.csv',index=False) \n",
    "frame5=pd.DataFrame(model.bn1.bias.data.numpy())  #存每个epoch的sigma和theta\n",
    "frame5.to_csv('C:/results/bn1bias.csv',index=False) \n",
    "frame6=pd.DataFrame(model.bn1.weight.data.numpy())  #存每个epoch的sigma和theta\n",
    "frame6.to_csv('C:/results/bn1weight.csv',index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccfd5d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.conv1.weight.detach().numpy()  #存所有的卷积核\n",
    "b = a.reshape(-1,9) #把三维的卷积核转化为9*16的矩阵进行存储\n",
    "np.savetxt('conv1weight.csv',b,delimiter=',')\n",
    "\n",
    "a = model.conv1.bias.detach().numpy() #存卷积层的偏执\n",
    "b = a.reshape(-1,1)\n",
    "np.savetxt('conv1bias.csv',b,delimiter=',')\n",
    "\n",
    "\n",
    "a = model.custom1.sigma.detach().numpy()  # 存归一化的系数\n",
    "b = a.reshape(-1,1)\n",
    "np.savetxt('custom1sigma.csv',b,delimiter=',')\n",
    "\n",
    "a = model.custom1.theta.detach().numpy()\n",
    "b = a.reshape(-1,1)\n",
    "np.savetxt('custom1theta.csv',b,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44a58ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.5915,  2.0098, -0.4391, -0.2575, -1.1112, -0.8826, -0.2765, -0.9498,\n",
      "         -0.0124,  0.4396,  1.0969, -1.0753, -0.2512,  0.0423,  0.0092, -3.0388,\n",
      "          0.0384,  0.0942, -0.6420,  1.1186, -0.4892, -1.2306,  0.0102, -0.4254,\n",
      "          0.4124,  0.6332,  0.1364,  1.2235,  1.7544,  0.1135,  0.2693,  0.0080,\n",
      "          0.0132,  0.0093,  0.7880,  0.4676,  0.1194, -0.0104, -0.0487, -0.5085,\n",
      "         -1.9784, -1.0716,  1.1823, -0.8316,  0.5135, -0.9583,  0.4216, -0.0114,\n",
      "          1.5141, -1.4576,  0.5765, -0.2202, -0.1937, -0.0082,  0.9217, -0.0119,\n",
      "         -1.3005,  0.4570, -1.2840,  1.0853,  0.0068, -0.1302, -0.0871, -0.0089,\n",
      "         -4.0709, -0.9054, -0.3224, -0.8944,  1.5545, -0.0063, -0.6245, -1.3657,\n",
      "         -1.2551,  0.4388,  1.0861,  0.0619, -0.5314,  0.5670,  0.2596,  0.4339,\n",
      "          1.8044,  0.7537,  0.9441, -0.5992, -0.2753,  0.0112,  0.1684, -1.5292,\n",
      "         -0.0817, -0.3967, -0.0768,  0.6471, -0.4027, -0.3745,  1.4199, -0.0794,\n",
      "         -0.7943, -1.6215,  0.7874,  0.9480, -0.0228, -1.2605, -0.2716, -0.2792,\n",
      "          0.8896, -0.6388, -2.1933,  0.0497, -1.5889,  0.2108, -0.9471,  0.0083,\n",
      "         -0.0858,  1.6395, -0.8504,  1.3121,  0.5916, -0.7845,  0.0081,  0.9429,\n",
      "         -0.7118, -0.9177,  1.8386, -0.0139,  0.0140,  0.0104, -1.5941,  0.5587,\n",
      "          0.1030,  0.6481,  0.6018, -2.2031, -0.4529, -0.0123, -1.0669,  0.0107,\n",
      "          1.1919, -1.0154,  1.5768,  1.4898,  0.0132,  0.3576, -0.7930, -0.6047,\n",
      "          0.0138,  0.0087,  0.0112, -1.1646, -0.0120,  2.1833,  2.7461,  0.0155,\n",
      "          0.0174, -0.0176,  0.0256,  1.0801, -0.0092,  0.0241, -1.2097, -0.9660,\n",
      "         -0.0325, -0.1234, -0.8224, -1.3835, -1.3139,  0.9923, -1.2298,  0.6416,\n",
      "         -1.3557, -0.4092, -0.8901, -0.4264,  1.1681,  1.6494, -0.5929,  0.2779,\n",
      "          0.6857,  0.0113, -0.4840,  0.5073,  0.0113, -0.4018,  0.9510, -1.1132,\n",
      "         -0.1171,  2.2345, -1.9328,  0.6531, -0.6513,  0.3861,  0.6914,  1.4559,\n",
      "          0.1410,  0.4416, -0.0120,  0.6065, -1.3586, -0.5466,  3.6640,  0.2218,\n",
      "         -0.0134, -1.8342, -1.3508, -0.4834, -2.3464,  1.0261,  0.6038, -0.0285,\n",
      "          0.8010,  0.2810, -0.3931, -0.0112,  0.6898, -0.8458, -1.0252, -0.8142,\n",
      "          1.6689,  0.0135,  1.2919,  0.0107, -0.3167,  0.9986, -0.0130, -0.6451,\n",
      "         -0.7961,  1.0801,  1.9586,  1.1582, -0.9652, -0.0231, -0.9717, -1.4620,\n",
      "          0.5340, -1.5317, -0.0102, -1.5216, -1.7404, -0.0144, -1.1067, -1.7303,\n",
      "          1.8042,  0.6218,  0.1441, -0.2343, -0.0880, -0.5959, -0.5014,  1.1418,\n",
      "          1.5380, -0.6929,  1.2385,  0.0121,  0.3939, -0.7913,  0.5732, -0.0413]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print (model.customFc1.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aee726bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 0.0052,  0.1707,  0.2916],\n",
      "          [-0.2794, -0.3592, -0.1685],\n",
      "          [ 0.2802, -0.0667,  0.0333]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2643, -0.3754,  0.0663],\n",
      "          [-0.0070, -0.0650,  0.0880],\n",
      "          [ 0.0579, -0.2710,  0.1737]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3234,  0.1599,  0.3626],\n",
      "          [ 0.0989,  0.0285, -0.1436],\n",
      "          [-0.2442, -0.1727, -0.1506]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2296, -0.1686, -0.2508],\n",
      "          [ 0.1457, -0.1463, -0.1180],\n",
      "          [-0.2138, -0.1083, -0.1363]]],\n",
      "\n",
      "\n",
      "        [[[-0.0337, -0.2428, -0.0622],\n",
      "          [-0.3229, -0.1352,  0.0137],\n",
      "          [ 0.3229,  0.0325,  0.1408]]],\n",
      "\n",
      "\n",
      "        [[[-0.2333,  0.0322, -0.1510],\n",
      "          [-0.3055,  0.2478,  0.0134],\n",
      "          [-0.2244, -0.2070,  0.1933]]],\n",
      "\n",
      "\n",
      "        [[[-0.2787,  0.2789, -0.1164],\n",
      "          [ 0.1156, -0.0906, -0.2311],\n",
      "          [ 0.1849,  0.1853,  0.2738]]],\n",
      "\n",
      "\n",
      "        [[[-0.3713, -0.2860, -0.1522],\n",
      "          [-0.2762,  0.1165, -0.3647],\n",
      "          [ 0.2344,  0.0846, -0.1037]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2262,  0.0849, -0.1770],\n",
      "          [-0.0153, -0.3198, -0.0918],\n",
      "          [ 0.1544, -0.2563, -0.3057]]],\n",
      "\n",
      "\n",
      "        [[[-0.1621, -0.0049, -0.1267],\n",
      "          [-0.2707,  0.2976, -0.0547],\n",
      "          [ 0.0725,  0.2296,  0.0225]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2532,  0.2468,  0.1527],\n",
      "          [ 0.1769,  0.0387,  0.2633],\n",
      "          [ 0.2602, -0.1635, -0.1744]]],\n",
      "\n",
      "\n",
      "        [[[-0.0427,  0.2084,  0.1233],\n",
      "          [ 0.2838,  0.2574,  0.2882],\n",
      "          [-0.1287, -0.1292,  0.0470]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1035, -0.2727, -0.2559],\n",
      "          [-0.0990, -0.1735,  0.2959],\n",
      "          [-0.3086, -0.1706, -0.2117]]],\n",
      "\n",
      "\n",
      "        [[[-0.2216,  0.2408, -0.0428],\n",
      "          [ 0.1946,  0.1170,  0.3056],\n",
      "          [-0.1245, -0.0675,  0.2298]]],\n",
      "\n",
      "\n",
      "        [[[-0.0906,  0.3586,  0.2910],\n",
      "          [-0.1502, -0.0893, -0.1668],\n",
      "          [-0.1657, -0.3223, -0.1195]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1945, -0.1384,  0.2700],\n",
      "          [-0.2921, -0.0846, -0.1622],\n",
      "          [ 0.0865,  0.0230,  0.2917]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print (model.conv1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35645244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
